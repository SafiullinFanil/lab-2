{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yL9L-4cwxZho"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SafiullinFanil/lab-2/blob/main/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snxSUWXD7q_p"
      },
      "source": [
        "## Домашнее задание по неделе 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-8OLsAV7wrQ"
      },
      "source": [
        "Как было рассказано на лекции, стохастический градиентый спуск сходится быстрее, чем полный, хотя и менее стабильно. В этом задании вам предлагается реализовать стохастический градиентный спуск и сравнить его с точным вычислением весов линейной модели по скорости работы и значению метрики качества.\n",
        "\n",
        "Задание курса https://courses.openedu.ru/courses/course-v1:hse+INTRML+fall_2020/course/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgQyWw5o7Nej"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGD1wQgMruJw"
      },
      "source": [
        "### Задание 0\n",
        "\n",
        "Реализуйте класс ```LinearRegressionSGD``` c обучением и и применением линейной регрессии, построенной с помощью стохастического градиентного спуска, с заданным интерфейсом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxdV27R9-uc"
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinearRegressionSGD(BaseEstimator):\n",
        "    def __init__(self, epsilon=1e-4, max_steps=100, w0=None, alpha=1e-4):\n",
        "        \"\"\"\n",
        "        epsilon: разница для нормы изменения весов \n",
        "        max_steps: максимальное количество шагов в градиентном спуске\n",
        "        w0: np.array (d,) - начальные веса\n",
        "        alpha: шаг обучения\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "        self.max_steps = max_steps\n",
        "        self.w0 = w0\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "        self.w_history = []\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        y: np.array (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        ## На каждом шаге градиентного спуска веса необходимо добавлять в w_history\n",
        "        \n",
        "        l, d = X.shape\n",
        "\n",
        "        if self.w0 is None:\n",
        "          self.w0 = np.zeros(d)\n",
        "\n",
        "        self.w = self.w0\n",
        "\n",
        "        for step in range(self.max_steps):\n",
        "          self.w_history.append(self.w)\n",
        "\n",
        "          w_new = self.w - self.alpha * self.calc_gradient(X, y)\n",
        "\n",
        "          if (np.linalg.norm(w_new - self.w) < self.epsilon):\n",
        "            break\n",
        "          \n",
        "          self.w = w_new\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        ---\n",
        "        output: np.array (l)\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.w is None:\n",
        "            raise Exception('Not trained yet')\n",
        "\n",
        "        return np.dot(X, self.w)\n",
        "    \n",
        "    def calc_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        y: np.array (l)\n",
        "        ---\n",
        "        output: np.array (d)\n",
        "        \"\"\"\n",
        "\n",
        "        l, d = X.shape\n",
        "\n",
        "        return (2/l) * np.dot(X.T,(np.dot(X, self.w) - y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNOm9-bXpdT3"
      },
      "source": [
        "Проверять работу мы будем на имеющемся в sklearn наборе данных boston: в нём нужно по информации о доме предсказать его стоимость."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c24JCwes9-pe"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_boston()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(X), y, test_size=0.3, random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIJwWnInXnr"
      },
      "source": [
        "### Задание 1\n",
        "\n",
        "Метрикой качества в нашей задаче будет MAPE - Mean Absolute Percentage Error. Реализуйте её с заданным интефейсом и вычислите \n",
        "```MAPE(y_test, y_0)```, где ```y_0 = (mean(y_test), mean(y_test), ...)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znoDavxyuLsi"
      },
      "source": [
        "def MAPE(y_true, y_pred):\n",
        "    \"\"\"\n",
        "        y_true: np.array (l)\n",
        "        y_pred: np.array (l)\n",
        "        ---\n",
        "        output: float [0, +inf)\n",
        "    \"\"\"\n",
        "    return (100/len(y_true)) * np.sum(abs((y_true - y_pred)/y_true))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6mTAykeojwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc23ea1-0bc9-418d-9389-c01f37d6ca80"
      },
      "source": [
        "y_0 = np.full_like(y_test, np.mean(y_test))\n",
        "\n",
        "MAPE(y_test, y_0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37.41588297684097"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nNy2ITxuMKf"
      },
      "source": [
        "### Задание 2 \n",
        "\n",
        "Обучите ```LinearRegressionSGD``` с базовыми параметрами на тренировочном наборе данных (```X_train```, ```y_train```), сделайте предсказание на тестовых данных ```X_test```, записав результат в переменную ```y_pred_sgd```  и вычислите ошибку MAPE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BIHwAwUvB-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f4bfe9-f592-4542-80cb-c284a316ab21"
      },
      "source": [
        "sgd = LinearRegressionSGD()\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "y_pred_sgd = sgd.predict(X_test)\n",
        "\n",
        "MAPE(y_test, y_pred_sgd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.750955469559937e+181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWappMdMtIPV"
      },
      "source": [
        "### Задание 3\n",
        "\n",
        "Вычислите веса по точной формуле, используя ```X_train``` и ```y_train```; предскажите с их помощью целевую переменную на ```X_test```, записав результат в переменную ```y_pred_lr``` и вычислите ошибку MAPE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjMUlPje9-k0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efde8d26-2bc5-42f1-e2b8-f12d2e60e3c9"
      },
      "source": [
        "l, d = X_train.shape\n",
        "alpha = 1e-4\n",
        "\n",
        "w = np.zeros(d)\n",
        "w -= alpha * (2/l) * np.dot(X_train.T,(np.dot(X_train, w) - y_train))\n",
        "\n",
        "y_pred_lr = np.dot(X_test, w)\n",
        "\n",
        "MAPE(y_test, y_pred_lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6432.848817330593"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL9L-4cwxZho"
      },
      "source": [
        "## Бонусное задание по неделе 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZFaUn7yx04u"
      },
      "source": [
        "До этого вы релизовывали модели, в которых не было штрафа за величину весов ```w```. Как было рассказано ранее в лекциях, это может привести к неустойчивости модели и переобучению. Чтобы избежать этих эффектов, предлагается добавить к оптимизируемому функционалу L2-норму весов; таким образом, будем решать задачу гребневой регрессии, Ridge:\n",
        "\n",
        "$$ \\frac{1}{l}(Xw-y)^T(Xw-y) +\\gamma||w||_2 \\rightarrow \\min_{w}. $$\n",
        "\n",
        "### Задание 11\n",
        "Реализуйте обучение такой модели в матричном виде (смотрите дополнительные материалы к этой неделе) с помощью стохастического градиентного спуска. Класс должен совпадать по набору реализованных функций с ```LinearRegressionVectorized```, разница будет лишь в параметре $\\gamma$, отвечающем за степень регуляризации. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEXqBqmGxWDz"
      },
      "source": [
        "class RidgeSGD(BaseEstimator):\n",
        "    def __init__(self, epsilon=1e-4, max_steps=1000, w0=None, alpha=1e-2, gamma=0):\n",
        "        \"\"\"\n",
        "        epsilon: разница для нормы изменения весов \n",
        "        max_steps: максимальное количество шагов в градиентном спуске\n",
        "        w0: np.array (d,) - начальные веса\n",
        "        alpha: шаг обучения\n",
        "        gamma: коэффициент регуляризации\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "        self.max_steps = max_steps\n",
        "        self.w0 = w0\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.w = None\n",
        "        self.w_history = []\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        y: np.array (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        ---\n",
        "        output: np.array (l)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    \n",
        "    def calc_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        y: np.array (l)\n",
        "        ---\n",
        "        output: np.array (d)\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t9rqXFu8Pq6"
      },
      "source": [
        "Так же, как и в основном задании, обучите модель с базовыми параметрами на тренировочных данных и сделайте прогноз y_pred_ridge. Выведите значение MAPE(y_test, y_pred_ridge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A2hak_A8QPO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}